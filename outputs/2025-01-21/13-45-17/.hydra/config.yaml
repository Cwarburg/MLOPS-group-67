model:
  model: distilbert
  pretrained-model: distilbert-base-uncased
  num_labels: 2
  batch_size: 2
train:
  batch_size: 32
  epochs: 100
  optimizer: AdamW
  lr: 1.0e-05
  eps: 0.005
data:
  path: data
